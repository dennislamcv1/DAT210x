{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAT210x - Programming with Python for DS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module5- Lab4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# You might need to import more modules here..\n",
    "# .. your code here ..\n",
    "\n",
    "matplotlib.style.use('ggplot') # Look Pretty\n",
    "c = ['red', 'green', 'blue', 'orange', 'yellow', 'brown']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can experiment with these parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_TYPE_TEXT = False    # If you'd like to see indices\n",
    "PLOT_VECTORS = True       # If you'd like to see your original features in P.C.-Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Convenience Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawVectors(transformed_features, components_, columns, plt):\n",
    "    num_columns = len(columns)\n",
    "\n",
    "    # This function will project your *original* feature (columns)\n",
    "    # onto your principal component feature-space, so that you can\n",
    "    # visualize how \"important\" each one was in the\n",
    "    # multi-dimensional scaling\n",
    "\n",
    "    # Scale the principal components by the max value in\n",
    "    # the transformed set belonging to that component\n",
    "    xvector = components_[0] * max(transformed_features[:,0])\n",
    "    yvector = components_[1] * max(transformed_features[:,1])\n",
    "\n",
    "    ## Visualize projections\n",
    "\n",
    "    # Sort each column by its length. These are your *original*\n",
    "    # columns, not the principal components.\n",
    "    important_features = { columns[i] : math.sqrt(xvector[i]**2 + yvector[i]**2) for i in range(num_columns) }\n",
    "    important_features = sorted(zip(important_features.values(), important_features.keys()), reverse=True)\n",
    "    print(\"Projected Features by importance:\\n\", important_features)\n",
    "\n",
    "    ax = plt.axes()\n",
    "\n",
    "    for i in range(num_columns):\n",
    "        # Use an arrow to project each original feature as a\n",
    "        # labeled vector on your principal component axes\n",
    "        plt.arrow(0, 0, xvector[i], yvector[i], color='b', width=0.0005, head_width=0.02, alpha=0.75, zorder=600000)\n",
    "        plt.text(xvector[i]*1.2, yvector[i]*1.2, list(columns)[i], color='b', alpha=0.75, zorder=600000)\n",
    "        \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doPCA(data, dimensions=2):\n",
    "    model = PCA(n_components=dimensions, svd_solver='randomized', random_state=7)\n",
    "    model.fit(data)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doKMeans(data, num_clusters=0):\n",
    "    # TODO: Do the KMeans clustering here, passing in the # of clusters parameter\n",
    "    # and fit it against your data. Then, return a tuple containing the cluster\n",
    "    # centers and the labels.\n",
    "    #\n",
    "    # Hint: Just like with doPCA above, you will have to create a variable called\n",
    "    # `model`, which will be a SKLearn K-Means model for this to work.\n",
    "    \n",
    "    \n",
    "    model = KMeans(n_clusters = clusters)\n",
    "    labels = model.fit_predict(data)\n",
    "  \n",
    "    return model.cluster_centers_, model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Load up the dataset. It may or may not have nans in it. Make sure you catch them and destroy them, by setting them to `0`. This is valid for this dataset, since if the value is missing, you can assume no money was spent on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Channel             0\n",
       "Region              0\n",
       "Fresh               0\n",
       "Milk                0\n",
       "Grocery             0\n",
       "Frozen              0\n",
       "Detergents_Paper    0\n",
       "Delicassen          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Wholesale customers data.csv')\n",
    "df\n",
    "df.isnull().sum() # Shows that there are no NaNs in the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As instructed, get rid of the `Channel` and `Region` columns, since you'll be investigating as if this were a single location wholesaler, rather than a national / international one. Leaving these fields in here would cause KMeans to examine and give weight to them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(labels = ['Channel', 'Region'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before unitizing / standardizing / normalizing your data in preparation for K-Means, it's a good idea to get a quick peek at it. You can do this using the `.describe()` method, or even by using the built-in pandas `df.plot.hist()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12ef2a10b48>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.describe()\n",
    "df.plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having checked out your data, you may have noticed there's a pretty big gap between the top customers in each feature category and the rest. Some feature scaling algorithms won't get rid of outliers for you, so it's a good idea to handle that manually---particularly if your goal is NOT to determine the top customers. \n",
    "\n",
    "After all, you can do that with a simple Pandas `.sort_values()` and not a machine learning clustering algorithm. From a business perspective, you're probably more interested in clustering your +/- 2 standard deviation customers, rather than the top and bottom customers.\n",
    "\n",
    "Remove top 5 and bottom 5 samples for each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = {}\n",
    "for col in df.columns:\n",
    "    # Bottom 5\n",
    "    sort = df.sort_values(by=col, ascending=True)\n",
    "    if len(sort) > 5: sort=sort[:5]\n",
    "    for index in sort.index: drop[index] = True # Just store the index once\n",
    "\n",
    "    # Top 5\n",
    "    sort = df.sort_values(by=col, ascending=False)\n",
    "    if len(sort) > 5: sort=sort[:5]\n",
    "    for index in sort.index: drop[index] = True # Just store the index once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop rows by index. We do this all at once in case there is a collision. This way, we don't end up dropping more rows than we have to, if there is a single row that satisfies the drop for multiple columns. Since there are 6 rows, if we end up dropping < 5*6*2 = 60 rows, that means there indeed were collisions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 42 Outliers...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fresh</th>\n",
       "      <th>Milk</th>\n",
       "      <th>Grocery</th>\n",
       "      <th>Frozen</th>\n",
       "      <th>Detergents_Paper</th>\n",
       "      <th>Delicassen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10996.231156</td>\n",
       "      <td>5144.090452</td>\n",
       "      <td>7091.711055</td>\n",
       "      <td>2639.721106</td>\n",
       "      <td>2562.974874</td>\n",
       "      <td>1278.736181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9933.042596</td>\n",
       "      <td>5057.406574</td>\n",
       "      <td>6923.019293</td>\n",
       "      <td>2974.246906</td>\n",
       "      <td>3608.176776</td>\n",
       "      <td>1220.745297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>258.000000</td>\n",
       "      <td>314.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3324.500000</td>\n",
       "      <td>1571.250000</td>\n",
       "      <td>2155.500000</td>\n",
       "      <td>749.750000</td>\n",
       "      <td>273.250000</td>\n",
       "      <td>409.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8257.500000</td>\n",
       "      <td>3607.500000</td>\n",
       "      <td>4573.000000</td>\n",
       "      <td>1526.000000</td>\n",
       "      <td>812.000000</td>\n",
       "      <td>946.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15828.500000</td>\n",
       "      <td>6953.250000</td>\n",
       "      <td>9922.250000</td>\n",
       "      <td>3370.250000</td>\n",
       "      <td>3841.500000</td>\n",
       "      <td>1752.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>53205.000000</td>\n",
       "      <td>29892.000000</td>\n",
       "      <td>39694.000000</td>\n",
       "      <td>17866.000000</td>\n",
       "      <td>19410.000000</td>\n",
       "      <td>7844.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Fresh          Milk       Grocery        Frozen  \\\n",
       "count    398.000000    398.000000    398.000000    398.000000   \n",
       "mean   10996.231156   5144.090452   7091.711055   2639.721106   \n",
       "std     9933.042596   5057.406574   6923.019293   2974.246906   \n",
       "min       37.000000    258.000000    314.000000     47.000000   \n",
       "25%     3324.500000   1571.250000   2155.500000    749.750000   \n",
       "50%     8257.500000   3607.500000   4573.000000   1526.000000   \n",
       "75%    15828.500000   6953.250000   9922.250000   3370.250000   \n",
       "max    53205.000000  29892.000000  39694.000000  17866.000000   \n",
       "\n",
       "       Detergents_Paper   Delicassen  \n",
       "count        398.000000   398.000000  \n",
       "mean        2562.974874  1278.736181  \n",
       "std         3608.176776  1220.745297  \n",
       "min           10.000000    11.000000  \n",
       "25%          273.250000   409.500000  \n",
       "50%          812.000000   946.500000  \n",
       "75%         3841.500000  1752.250000  \n",
       "max        19410.000000  7844.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Dropping {0} Outliers...\".format(len(drop)))\n",
    "df.drop(inplace=True, labels=drop.keys(), axis=0)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are you interested in?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Depending on what you're interested in, you might take a different approach to normalizing/standardizing your data.\n",
    " \n",
    "You should note that all columns left in the dataset are of the same unit. You might ask yourself, do I even need to normalize / standardize the data? The answer depends on what you're trying to accomplish. For instance, although all the units are the same (generic money unit), the price per item in your store isn't. There may be some cheap items and some expensive one. If your goal is to find out what items people tend to buy together but you didn't  \"unitize\" properly before running kMeans, the contribution of the lesser priced item would be dwarfed by the more expensive item. This is an issue of scale.\n",
    "\n",
    "For a great overview on a few of the normalization methods supported in SKLearn, please check out: https://stackoverflow.com/questions/30918781/right-function-for-normalizing-input-of-sklearn-svm\n",
    "\n",
    "Suffice to say, at the end of the day, you're going to have to know what question you want answered and what data you have available in order to select the best method for your purpose. Luckily, SKLearn's interfaces are easy to switch out so in the mean time, you can experiment with all of them and see how they alter your results.\n",
    "\n",
    "5-sec summary before you dive deeper online:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say your user spend a LOT. Normalization divides each item by the average overall amount of spending. Stated differently, your new feature is = the contribution of overall spending going into that particular item: \\$spent on feature / \\$overall spent by sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What % in the overall range of $spent by all users on THIS particular feature is the current sample's feature at? When you're dealing with all the same units, this will produce a near face-value amount. Be careful though: if you have even a single outlier, it can cause all your data to get squashed up in lower percentages.\n",
    "\n",
    "Imagine your buyers usually spend \\$100 on wholesale milk, but today only spent \\$20. This is the relationship you're trying to capture with MinMax. NOTE: MinMax doesn't standardize (std. dev.); it only normalizes / unitizes your feature, in the mathematical sense. MinMax can be used as an alternative to zero mean, unit variance scaling. [(sampleFeatureValue-min) / (max-min)] * (max-min) + min Where min and max are for the overall feature values for all samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to The Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un-comment just ***ONE*** of lines at a time and see how alters your results. Pay attention to the direction of the arrows, as well as their LENGTHS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#T = preprocessing.StandardScaler().fit_transform(df)\n",
    "#T = preprocessing.MinMaxScaler().fit_transform(df)\n",
    "#T = preprocessing.MaxAbsScaler().fit_transform(df)\n",
    "#T = preprocessing.Normalizer().fit_transform(df)\n",
    "T = df # No Change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Sometimes people perform PCA before doing KMeans, so that KMeans only operates on the most meaningful features. In our case, there are so few features that doing PCA ahead of time isn't really necessary, and you can do KMeans in feature space. But keep in mind you have the option to transform your data to bring down its dimensionality. If you take that route, then your Clusters will already be in PCA-transformed feature space, and you won't have to project them again for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KMeans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-5dbf2f086e9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Do KMeans\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mn_clusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcentroids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-19fda1e2cd9f>\u001b[0m in \u001b[0;36mdoKMeans\u001b[1;34m(data, num_clusters)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclusters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'KMeans' is not defined"
     ]
    }
   ],
   "source": [
    "# Do KMeans\n",
    "n_clusters = 3\n",
    "centroids, labels = doKMeans(T, n_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out your centroids. They're currently in feature-space, which is good. Print them out before you transform them into PCA space for viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've clustered our KMeans, let's do PCA, using it as a tool to visualize the results. Project the centroids as well as the samples into the new 2D feature space for visualization purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_pca = doPCA(T)\n",
    "T = display_pca.transform(T)\n",
    "CC = display_pca.transform(centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize all the samples. Give them the color of their cluster label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "if PLOT_TYPE_TEXT:\n",
    "    # Plot the index of the sample, so you can further investigate it in your dset\n",
    "    for i in range(len(T)): ax.text(T[i,0], T[i,1], df.index[i], color=c[labels[i]], alpha=0.75, zorder=600000)\n",
    "    ax.set_xlim(min(T[:,0])*1.2, max(T[:,0])*1.2)\n",
    "    ax.set_ylim(min(T[:,1])*1.2, max(T[:,1])*1.2)\n",
    "else:\n",
    "    # Plot a regular scatter plot\n",
    "    sample_colors = [ c[labels[i]] for i in range(len(T)) ]\n",
    "    ax.scatter(T[:, 0], T[:, 1], c=sample_colors, marker='o', alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the Centroids as X's, and label them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.scatter(CC[:, 0], CC[:, 1], marker='x', s=169, linewidths=3, zorder=1000, c=c)\n",
    "for i in range(len(centroids)):\n",
    "    ax.text(CC[i, 0], CC[i, 1], str(i), zorder=500010, fontsize=18, color=c[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature vectors for investigation:\n",
    "if PLOT_VECTORS:\n",
    "    drawVectors(T, display_pca.components_, df.columns, plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the cluster label back into the dataframe and display it:\n",
    "df['label'] = pd.Series(labels, index=df.index)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "58px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
